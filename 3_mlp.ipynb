{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Character Level Multi Layer Perceptron Language Model**\n",
    "This is inspired by [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '<',\n",
       " 27: '>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create lookup table for converting characters to indices\n",
    "chars = sorted(list(set(''.join(words)))) # all unique characters in the dataset\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # string to index\n",
    "\n",
    "# manually enumerate start and end token since they are not visible in the dataset\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "stoi[start_token] = 0\n",
    "stoi[end_token] = len(stoi)\n",
    "\n",
    "# total number of unique characters plus start and end token\n",
    "chars_count = len(stoi)\n",
    "\n",
    "# index to string\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "itos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset\n",
    "Based on given list of words creates input tensor with sequence of characters  with length equal to 'context_length' and target tensor with next character in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, context_length=3):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [stoi[start_token]] * context_length # context window padded with start token\n",
    "        for ch in w + end_token:\n",
    "            ix = stoi[ch]\n",
    "            X.append(context) # for given context ...\n",
    "            Y.append(ix) # ... the next character is the target\n",
    "            context = context[1:] + [ix] # crop and append - sliding window\n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([5, 3]), X.dtype = torch.int64\n",
      "Y.shape = torch.Size([5]), Y.dtype = torch.int64\n",
      "For the first word in the dataset: <emma>\n",
      "X[0] = [0, 0, 0] = <<< ---and target char is--> Y[0] = 5 = e\n",
      "X[1] = [0, 0, 5] = <<e ---and target char is--> Y[1] = 13 = m\n",
      "X[2] = [0, 5, 13] = <em ---and target char is--> Y[2] = 13 = m\n",
      "X[3] = [5, 13, 13] = emm ---and target char is--> Y[3] = 1 = a\n",
      "X[4] = [13, 13, 1] = mma ---and target char is--> Y[4] = 27 = >\n"
     ]
    }
   ],
   "source": [
    "# X (input) and Y (output/target/label) tensors\n",
    "X, Y = build_dataset(words[:1])\n",
    "print(f'X.shape = {X.shape}, X.dtype = {X.dtype}')\n",
    "print(f'Y.shape = {Y.shape}, Y.dtype = {Y.dtype}')\n",
    "\n",
    "print(f'For the first word in the dataset: {start_token + words[0] + end_token}')\n",
    "for i in range(len(X)):\n",
    "    print(f\"X[{i}] = {X[i].tolist()} = {''.join([itos[j] for j in X[i].tolist()])}\"\n",
    "          f\" ---and target char is--> Y[{i}] = {Y[i]} = {itos[Y[i].item()]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding using index C[15]=tensor([0.6430, 1.5565])\n",
      "Embedding using one-hot encoding and matrix multiplication=tensor([[0.6430, 1.5565]])\n",
      "Embedding for sequence of three characters C[torch.tensor([15, 20, 5])]=\n",
      "tensor([[ 0.6430,  1.5565],\n",
      "        [ 0.3051, -1.4014],\n",
      "        [-0.8937, -0.1624]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix of embeddings\n",
    "C = torch.randn((chars_count, 2)) # 2-dimensional embeddings\n",
    "\n",
    "# Example of embedding single character with index 15\n",
    "# since our matrix of embeddings is the same size as the number of characters\n",
    "# we can simply use the index of the character to get its embedding\n",
    "example_embedding = C[15]\n",
    "print(f'Embedding using index C[15]={example_embedding}')\n",
    "\n",
    "# Alternative approach would be to one-hot encode the character and then multiply it by the embedding matrix\n",
    "# this will give same result because encoded vector will have only one non-zero value equal to 1\n",
    "# and this will simply act as a mask for the embedding matrix\n",
    "example_embedding = F.one_hot(torch.tensor([15]), num_classes=chars_count).float() @ C\n",
    "print(f'Embedding using one-hot encoding and matrix multiplication={example_embedding}')\n",
    "\n",
    "# For rest of of notebook I will use index based approach because it is more efficient\n",
    "# also thanks to python semantics we can easly retrieve embedding for whole sequence of characters\n",
    "# by indexing with list or tensor of integers\n",
    "example_embedding = C[torch.tensor([15, 20, 5])]\n",
    "print(f'Embedding for sequence of three characters C[torch.tensor([15, 20, 5])]=\\n{example_embedding}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of C[X].shape=torch.Size([5, 3, 2])\n",
      "And C[X] =\n",
      "tensor([[[ 1.8825, -1.1100],\n",
      "         [ 1.8825, -1.1100],\n",
      "         [ 1.8825, -1.1100]],\n",
      "\n",
      "        [[ 1.8825, -1.1100],\n",
      "         [ 1.8825, -1.1100],\n",
      "         [-0.8937, -0.1624]],\n",
      "\n",
      "        [[ 1.8825, -1.1100],\n",
      "         [-0.8937, -0.1624],\n",
      "         [ 0.1640, -1.3959]],\n",
      "\n",
      "        [[-0.8937, -0.1624],\n",
      "         [ 0.1640, -1.3959],\n",
      "         [ 0.1640, -1.3959]],\n",
      "\n",
      "        [[ 0.1640, -1.3959],\n",
      "         [ 0.1640, -1.3959],\n",
      "         [ 0.5766, -1.0162]]])\n",
      "We can also access individual ebeddings for given sequence, for example C[X][0] =\n",
      "tensor([[ 1.8825, -1.1100],\n",
      "        [ 1.8825, -1.1100],\n",
      "        [ 1.8825, -1.1100]])\n",
      "Or even embedding for individual character in sequence, for example C[X][0][0] =\n",
      "tensor([ 1.8825, -1.1100])\n"
     ]
    }
   ],
   "source": [
    "# What is even more mind blowing is that we can index using multidimensional tensor!\n",
    "# in this exaple where X is 5x3 tensor, we will get 5x3x2 tensor with embeddings for each character in each sequence in third dimension\n",
    "print(f'Shape of C[X].shape={C[X].shape}')\n",
    "print(f'And C[X] =\\n{C[X]}')\n",
    "print(f'We can also access individual ebeddings for given sequence, for example C[X][0] =\\n{C[X][0]}')\n",
    "print(f'Or even embedding for individual character in sequence, for example C[X][0][0] =\\n{C[X][0][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings for imput sequences\n",
    "X_embedded = C[X]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP input and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer_size=6\n",
      "hidden_layer_size=100\n",
      "W1.shape=torch.Size([6, 100])\n",
      "b1.shape=torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Number of inputs for first layer\n",
    "# context length * number of embeddings for each character\n",
    "# in our case 3 * 2 = 6 which is the same as shape[1] * shape[2] of X_embedded\n",
    "# and we will pass embeddings for each character in the sequence as input to the network\n",
    "input_layer_size = X_embedded.shape[1] * X_embedded.shape[2]\n",
    "print(f'input_layer_size={input_layer_size}')\n",
    "\n",
    "# number of neurons in hidden layer\n",
    "hidden_layer_size = 100\n",
    "print(f'hidden_layer_size={hidden_layer_size}')\n",
    "\n",
    "# W1 represents neural network weights between input and hidden layer\n",
    "W1 = torch.randn((input_layer_size, hidden_layer_size))\n",
    "print(f'W1.shape={W1.shape}')\n",
    "\n",
    "# b1 represents bias for hidden layer\n",
    "b1 = torch.randn(hidden_layer_size)\n",
    "print(f'b1.shape={b1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embedded[:, 0, :]=\n",
      "tensor([[ 1.8825, -1.1100],\n",
      "        [ 1.8825, -1.1100],\n",
      "        [ 1.8825, -1.1100],\n",
      "        [-0.8937, -0.1624],\n",
      "        [ 0.1640, -1.3959]])\n",
      "\n",
      "torch.concat + tensor list=\n",
      "tensor([[ 1.8825, -1.1100,  1.8825, -1.1100,  1.8825, -1.1100],\n",
      "        [ 1.8825, -1.1100,  1.8825, -1.1100, -0.8937, -0.1624],\n",
      "        [ 1.8825, -1.1100, -0.8937, -0.1624,  0.1640, -1.3959],\n",
      "        [-0.8937, -0.1624,  0.1640, -1.3959,  0.1640, -1.3959],\n",
      "        [ 0.1640, -1.3959,  0.1640, -1.3959,  0.5766, -1.0162]])\n",
      "\n",
      "torch.concat + torch.unbind=\n",
      "tensor([[ 1.8825, -1.1100,  1.8825, -1.1100,  1.8825, -1.1100],\n",
      "        [ 1.8825, -1.1100,  1.8825, -1.1100, -0.8937, -0.1624],\n",
      "        [ 1.8825, -1.1100, -0.8937, -0.1624,  0.1640, -1.3959],\n",
      "        [-0.8937, -0.1624,  0.1640, -1.3959,  0.1640, -1.3959],\n",
      "        [ 0.1640, -1.3959,  0.1640, -1.3959,  0.5766, -1.0162]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In order to perform matrix multiplication between input and weights we need to \n",
    "# transform input tensor which is 5x3x2 into 5x6 to fit into neural network\n",
    "\n",
    "# This is how to access all embeddings for first character in each sequence\n",
    "print(f'X_embedded[:, 0, :]=\\n{X_embedded[:, 0, :]}\\n')\n",
    "\n",
    "# Knowing above we can concatenate all embeddings for each character in each sequence\n",
    "print(f'torch.concat + tensor list=\\n{torch.concat([X_embedded[:, i, :] for i in range(X_embedded.shape[1])], dim=1)}\\n')\n",
    "\n",
    "# We can also improve it by using torch.unbind which will return list of tensors and yield same result as for loop\n",
    "print(f'torch.concat + torch.unbind=\\n{torch.concat(torch.unbind(X_embedded, dim=1), dim=1)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape=torch.Size([16]), a=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\n",
      "a.view(2, 8)=\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15]])\n",
      "\n",
      "a.view(4, 4)=\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "\n",
      "a.view(2, 2, 4)=\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# But it turns out there is another great function in pytorch called torch.view\n",
    "# Lets have some fun with it\n",
    "\n",
    "a = torch.arange(16) # 1D tensor with 16 elements\n",
    "print(f'a.shape={a.shape}, a={a}\\n')\n",
    "print(f'a.view(2, 8)=\\n{a.view(2, 8)}\\n')\n",
    "print(f'a.view(4, 4)=\\n{a.view(4, 4)}\\n')\n",
    "print(f'a.view(2, 2, 4)=\\n{a.view(2, 2, 4)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embedded_flat.shape=torch.Size([5, 6])\n",
      "\n",
      "X_embedded_flat=\n",
      "tensor([[ 1.8825, -1.1100,  1.8825, -1.1100,  1.8825, -1.1100],\n",
      "        [ 1.8825, -1.1100,  1.8825, -1.1100, -0.8937, -0.1624],\n",
      "        [ 1.8825, -1.1100, -0.8937, -0.1624,  0.1640, -1.3959],\n",
      "        [-0.8937, -0.1624,  0.1640, -1.3959,  0.1640, -1.3959],\n",
      "        [ 0.1640, -1.3959,  0.1640, -1.3959,  0.5766, -1.0162]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Knowing above we can use torch.view to transform our 5x3x2 tensor into 5x6\n",
    "# Basically we just keep first dimension and merge all other dimensions into one\n",
    "# Also it looks like 6 is the only possible value for second dimension because 3*2=6\n",
    "# Knowing that we can use -1 for second dimension and pytorch will figure out the rest\n",
    "X_embedded_flat = X_embedded.view(-1, 6)\n",
    "print(f'X_embedded_flat.shape={X_embedded_flat.shape}\\n')\n",
    "print(f'X_embedded_flat=\\n{X_embedded_flat}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First layer multiplication to get outputs from hidden layer\n",
    "Our current desing of network is: 6 inputs which represents 2 dimensional embeddings for 3 character long sequence that go into 100 neurons in hidden layer and what we get is 100 outputs for each input sequence.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.shape=torch.Size([5, 100])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h = torch.tanh(X_embedded_flat @ W1 + b1) \n",
    "# + b1 is broadcasted to match shape of X_embedded_flat @ W1\n",
    "# 5, 100\n",
    "# 1, 100 - broadcasting will align on the right, create fake dimension on the left\n",
    "# and will be copy b1 values 5 times to match shape of (X_embedded_flat @ W1) and perform elementwise addition\n",
    "# and will result of adding 5x100 matrix to 5x100 matrix with copied values from b1\n",
    "print(f'h.shape={h.shape}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "Takes 100 outputs from hidden layer and produces output that represent one_hot_encoding for our characters set for each input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((hidden_layer_size, chars_count))\n",
    "b2 = torch.randn(chars_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forwad pass for output layer\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# calculate output probabilities (the old way, manualy calculating softmax)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "# but there is faster way to calculate probabilities - use torch.nn.functional.softmax\n",
    "# and this is equivalent to above calculation\n",
    "probs = F.softmax(logits, dim=1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1368e-08, 3.6839e-04, 1.5063e-02, 6.5854e-12, 1.5507e-08, 6.2123e-06,\n",
       "        8.6732e-09, 1.9709e-02, 4.6237e-11, 2.5763e-07, 4.1929e-13, 2.1549e-06,\n",
       "        1.1461e-10, 5.0471e-08, 2.1204e-07, 1.5089e-07, 5.6234e-16, 2.9176e-08,\n",
       "        3.0949e-05, 5.8218e-06, 3.3787e-08, 9.6467e-01, 3.2289e-11, 6.1506e-06,\n",
       "        1.7879e-05, 1.2172e-08, 8.9791e-05, 2.9898e-05])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And what we see here is output character probabilities for second input sequence\n",
    "probs[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training = 15.981466293334961\n"
     ]
    }
   ],
   "source": [
    "# iterator over input sequences\n",
    "it = torch.arange(X_embedded.shape[0])\n",
    "\n",
    "# extract probabilities for target characters\n",
    "target_probs = probs[it, Y]\n",
    "\n",
    "# calculate loss as average negative log likelihood\n",
    "loss = -torch.log(target_probs).mean()\n",
    "\n",
    "print(f'Loss before training = {loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep it short and clean without outputs\n",
    "Above code show detail example of how MLP language model works with one input word.  Since I already went step by step with relevant outputs and example is clear next step is to make this code compact and efficient so it can be used for training and evaluation of language model for whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters = 33288\n",
      "Training data set size = 228146\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "context_length = 4\n",
    "embedding_size = 20\n",
    "hidden_layer_size = 300\n",
    "\n",
    "input_layer_size = context_length * embedding_size\n",
    "\n",
    "# Build dataset for training\n",
    "X, Y = build_dataset(words, context_length)\n",
    "\n",
    "# Create embedding lookup table with size chars_count\n",
    "C = torch.randn((chars_count, embedding_size))\n",
    "\n",
    "# Create first neuron layer\n",
    "W1 = torch.randn((input_layer_size, hidden_layer_size))\n",
    "b1 = torch.randn(hidden_layer_size)\n",
    "\n",
    "# Create second neuron layer\n",
    "W2 = torch.randn((hidden_layer_size, chars_count)) * 0.01 # small number instead of zero because we want some entropy for symetry breaking\n",
    "b2 = torch.zeros(chars_count)\n",
    "\n",
    "# Parameters of the network\n",
    "# high number of parameters may result in overfitting this may happen when we have more parameters than data\n",
    "# in this case we may even go to loss=0 but it will not generalize well to new data, it will simply memorize training data\n",
    "params = [C, W1, b1, W2, b2]\n",
    "\n",
    "# Set requires_grad=True for all parameters\n",
    "for p in params:\n",
    "    p.requires_grad_(True)\n",
    "    \n",
    "print(f'Number of parameters = {sum([p.numel() for p in params])}')\n",
    "print(f'Training data set size = {X.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Introduce minibatch for training. When training with minibatches we don't fit all ouf our training data at once during each of training loop to calculate gradient and perform parameter update. Instead we split our training data and each epoch we take randomly selected minibatch of data and perform parameter update. This way we don't get exact gradient but we get good approximation of gradient and we can train our model much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of epochs that took place\n",
    "epochs_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21000, loss = 2.2271368503570557\n",
      "Epoch 21100, loss = 2.168222427368164\n",
      "Epoch 21200, loss = 2.2955281734466553\n",
      "Epoch 21300, loss = 2.2815043926239014\n",
      "Epoch 21400, loss = 2.2216453552246094\n",
      "Epoch 21500, loss = 2.2689788341522217\n",
      "Epoch 21600, loss = 2.2126717567443848\n",
      "Epoch 21700, loss = 2.2340569496154785\n",
      "Epoch 21800, loss = 2.2374818325042725\n",
      "Epoch 21900, loss = 2.196133852005005\n"
     ]
    }
   ],
   "source": [
    "steps = 1000\n",
    "mini_batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "for _ in range(steps):\n",
    "    # Minibatch\n",
    "    # create tensor with random indices between 0 and X.shape[0] with size mini_batch_size\n",
    "    # there is much better way to do it in pytorch, but this is just for learning purposes\n",
    "    # also in this case we may train on the same input sequence multiple times\n",
    "    ix = torch.randint(0, X.shape[0], (mini_batch_size,))    \n",
    "\n",
    "    # Forward pass\n",
    "    X_embedded = C[X[ix]]\n",
    "    h1 = torch.tanh(X_embedded.view(mini_batch_size, -1) @ W1 + b1)\n",
    "    logits = h1 @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    \n",
    "    # Print loss\n",
    "    if epochs_counter % 100 == 0:\n",
    "        print(f'Epoch {epochs_counter}, loss = {loss}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Update parameters\n",
    "    for p in params:\n",
    "        p.data -= learning_rate * p.grad\n",
    "        p.grad.data.zero_()\n",
    "        \n",
    "    epochs_counter += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final loss\n",
    "Final loss is less than in case of bigrams (~2,454) which was expected. If we increase number of parameters results will be even better but it looks like waste of time on this concept lets move to more advanced networks ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on whole dataset after training for 22000 epochs = 2.2438600063323975\n"
     ]
    }
   ],
   "source": [
    "# Final loss\n",
    "h1 = torch.tanh(C[X].view(X.shape[0], -1) @ W1 + b1)\n",
    "loss = F.cross_entropy(h1 @ W2 + b2, Y)\n",
    "print(f'Loss on whole dataset after training for {epochs_counter} epochs = {loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer why to use cross entropy vs manual average negative loss likelihood calculation\n",
    "\n",
    "Calculate mean negative log likelihood based on softmax made of logits\n",
    "using torch.nn.functional.cross_entropy() which is give same result as:\n",
    "```python\n",
    "counts = logits.exp() # this may cause trobles if logits are too big we will get nan, since exp for big numbers is inf\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "loss = -torch.log(probs[range(len(Y)), Y]).mean()\n",
    "```\n",
    "- but much meory efficient because we do not create intermediate tensors\n",
    "- and it is safe in case of possible big logits because internally it ofsets them by max logit value\n",
    "- and much faster and... also makes backpropagation easier :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tana\n",
      "glyiah\n",
      "naztt\n",
      "oahnoe\n",
      "shye\n",
      "jahbe\n",
      "rallrish\n",
      "raylya\n",
      "ayjas\n",
      "ania\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    out = []\n",
    "    \n",
    "    context = [stoi[start_token]] * context_length\n",
    "    while True:\n",
    "        embedding = C[torch.tensor([context])]\n",
    "        # we always consider only one sequence at a time, so we can set second dimension to -1 and pytorch will figure out the rest\n",
    "        embedding_flat = embedding.view(1, -1)\n",
    "        h1 = torch.tanh(embedding_flat @ W1 + b1)\n",
    "        logits = h1 @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # sample next character\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        context = context[1:] + [ix]\n",
    "        if ix == stoi[end_token]:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
