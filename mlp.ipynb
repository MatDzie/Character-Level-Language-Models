{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Character Level Multi Layer Perceptron Language Model**\n",
    "This is inspired by [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '<',\n",
       " 27: '>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create lookup table for converting characters to indices\n",
    "chars = sorted(list(set(''.join(words)))) # all unique characters in the dataset\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # string to index\n",
    "\n",
    "# manually enumerate start and end token since they are not visible in the dataset\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "stoi[start_token] = 0\n",
    "stoi[end_token] = len(stoi)\n",
    "\n",
    "# total number of unique characters plus start and end token\n",
    "chars_count = len(stoi)\n",
    "\n",
    "# index to string\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "itos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset\n",
    "Based on given list of words creates input tensor with sequence of characters  with length equal to 'context_length' and target tensor with next character in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, context_length=3):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [stoi[start_token]] * context_length # context window padded with start token\n",
    "        for ch in w + end_token:\n",
    "            ix = stoi[ch]\n",
    "            X.append(context) # for given context ...\n",
    "            Y.append(ix) # ... the next character is the target\n",
    "            context = context[1:] + [ix] # crop and append - sliding window\n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = torch.Size([5, 3]), X.dtype = torch.int64\n",
      "Y.shape = torch.Size([5]), Y.dtype = torch.int64\n",
      "For the first word in the dataset: <emma>\n",
      "X[0] = [0, 0, 0] = <<< ---and target char is--> Y[0] = 5 = e\n",
      "X[1] = [0, 0, 5] = <<e ---and target char is--> Y[1] = 13 = m\n",
      "X[2] = [0, 5, 13] = <em ---and target char is--> Y[2] = 13 = m\n",
      "X[3] = [5, 13, 13] = emm ---and target char is--> Y[3] = 1 = a\n",
      "X[4] = [13, 13, 1] = mma ---and target char is--> Y[4] = 27 = >\n"
     ]
    }
   ],
   "source": [
    "# X (input) and Y (output/target/label) tensors\n",
    "X, Y = build_dataset(words[:1])\n",
    "print(f'X.shape = {X.shape}, X.dtype = {X.dtype}')\n",
    "print(f'Y.shape = {Y.shape}, Y.dtype = {Y.dtype}')\n",
    "\n",
    "print(f'For the first word in the dataset: {start_token + words[0] + end_token}')\n",
    "for i in range(len(X)):\n",
    "    print(f\"X[{i}] = {X[i].tolist()} = {''.join([itos[j] for j in X[i].tolist()])}\"\n",
    "          f\" ---and target char is--> Y[{i}] = {Y[i]} = {itos[Y[i].item()]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding using index C[15]=tensor([0.3793, 2.1740])\n",
      "Embedding using one-hot encoding and matrix multiplication=tensor([[0.3793, 2.1740]])\n",
      "Embedding for sequence of three characters C[torch.tensor([15, 20, 5])]=\n",
      "tensor([[ 0.3793,  2.1740],\n",
      "        [-0.7536, -0.0077],\n",
      "        [ 2.1247,  0.4705]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix of embeddings\n",
    "C = torch.randn((chars_count, 2)) # 2-dimensional embeddings\n",
    "\n",
    "# Example of embedding single character with index 15\n",
    "# since our matrix of embeddings is the same size as the number of characters\n",
    "# we can simply use the index of the character to get its embedding\n",
    "example_embedding = C[15]\n",
    "print(f'Embedding using index C[15]={example_embedding}')\n",
    "\n",
    "# Alternative approach would be to one-hot encode the character and then multiply it by the embedding matrix\n",
    "# this will give same result because encoded vector will have only one non-zero value equal to 1\n",
    "# and this will simply act as a mask for the embedding matrix\n",
    "example_embedding = F.one_hot(torch.tensor([15]), num_classes=chars_count).float() @ C\n",
    "print(f'Embedding using one-hot encoding and matrix multiplication={example_embedding}')\n",
    "\n",
    "# For rest of of notebook I will use index based approach because it is more efficient\n",
    "# also thanks to python semantics we can easly retrieve embedding for whole sequence of characters\n",
    "# by indexing with list or tensor of integers\n",
    "example_embedding = C[torch.tensor([15, 20, 5])]\n",
    "print(f'Embedding for sequence of three characters C[torch.tensor([15, 20, 5])]=\\n{example_embedding}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of C[X].shape=torch.Size([5, 3, 2])\n",
      "And C[X] =\n",
      "tensor([[[ 0.1912,  0.2553],\n",
      "         [ 0.1912,  0.2553],\n",
      "         [ 0.1912,  0.2553]],\n",
      "\n",
      "        [[ 0.1912,  0.2553],\n",
      "         [ 0.1912,  0.2553],\n",
      "         [ 2.1247,  0.4705]],\n",
      "\n",
      "        [[ 0.1912,  0.2553],\n",
      "         [ 2.1247,  0.4705],\n",
      "         [-0.3938, -0.0033]],\n",
      "\n",
      "        [[ 2.1247,  0.4705],\n",
      "         [-0.3938, -0.0033],\n",
      "         [-0.3938, -0.0033]],\n",
      "\n",
      "        [[-0.3938, -0.0033],\n",
      "         [-0.3938, -0.0033],\n",
      "         [ 0.6780, -2.8017]]])\n",
      "We can also access individual ebeddings for given sequence, for example C[X][0] =\n",
      "tensor([[0.1912, 0.2553],\n",
      "        [0.1912, 0.2553],\n",
      "        [0.1912, 0.2553]])\n",
      "Or even embedding for individual character in sequence, for example C[X][0][0] =\n",
      "tensor([0.1912, 0.2553])\n"
     ]
    }
   ],
   "source": [
    "# What is even more mind blowing is that we can index using multidimensional tensor!\n",
    "# in this exaple where X is 5x3 tensor, we will get 5x3x2 tensor with embeddings for each character in each sequence in third dimension\n",
    "print(f'Shape of C[X].shape={C[X].shape}')\n",
    "print(f'And C[X] =\\n{C[X]}')\n",
    "print(f'We can also access individual ebeddings for given sequence, for example C[X][0] =\\n{C[X][0]}')\n",
    "print(f'Or even embedding for individual character in sequence, for example C[X][0][0] =\\n{C[X][0][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings for imput sequences\n",
    "X_embedded = C[X]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP input and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer_size=6\n",
      "hidden_layer_size=100\n",
      "W1.shape=torch.Size([6, 100])\n",
      "b1.shape=torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Number of inputs for first layer\n",
    "# context length * number of embeddings for each character\n",
    "# in our case 3 * 2 = 6 which is the same as shape[1] * shape[2] of X_embedded\n",
    "# and we will pass embeddings for each character in the sequence as input to the network\n",
    "input_layer_size = X_embedded.shape[1] * X_embedded.shape[2]\n",
    "print(f'input_layer_size={input_layer_size}')\n",
    "\n",
    "# number of neurons in hidden layer\n",
    "hidden_layer_size = 100\n",
    "print(f'hidden_layer_size={hidden_layer_size}')\n",
    "\n",
    "# W1 represents neural network weights between input and hidden layer\n",
    "W1 = torch.randn((input_layer_size, hidden_layer_size))\n",
    "print(f'W1.shape={W1.shape}')\n",
    "\n",
    "# b1 represents bias for hidden layer\n",
    "b1 = torch.randn(hidden_layer_size)\n",
    "print(f'b1.shape={b1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embedded[:, 0, :]=\n",
      "tensor([[ 0.1912,  0.2553],\n",
      "        [ 0.1912,  0.2553],\n",
      "        [ 0.1912,  0.2553],\n",
      "        [ 2.1247,  0.4705],\n",
      "        [-0.3938, -0.0033]])\n",
      "\n",
      "torch.concat + tensor list=\n",
      "tensor([[ 0.1912,  0.2553,  0.1912,  0.2553,  0.1912,  0.2553],\n",
      "        [ 0.1912,  0.2553,  0.1912,  0.2553,  2.1247,  0.4705],\n",
      "        [ 0.1912,  0.2553,  2.1247,  0.4705, -0.3938, -0.0033],\n",
      "        [ 2.1247,  0.4705, -0.3938, -0.0033, -0.3938, -0.0033],\n",
      "        [-0.3938, -0.0033, -0.3938, -0.0033,  0.6780, -2.8017]])\n",
      "\n",
      "torch.concat + torch.unbind=\n",
      "tensor([[ 0.1912,  0.2553,  0.1912,  0.2553,  0.1912,  0.2553],\n",
      "        [ 0.1912,  0.2553,  0.1912,  0.2553,  2.1247,  0.4705],\n",
      "        [ 0.1912,  0.2553,  2.1247,  0.4705, -0.3938, -0.0033],\n",
      "        [ 2.1247,  0.4705, -0.3938, -0.0033, -0.3938, -0.0033],\n",
      "        [-0.3938, -0.0033, -0.3938, -0.0033,  0.6780, -2.8017]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In order to perform matrix multiplication between input and weights we need to \n",
    "# transform input tensor which is 5x3x2 into 5x6 to fit into neural network\n",
    "\n",
    "# This is how to access all embeddings for first character in each sequence\n",
    "print(f'X_embedded[:, 0, :]=\\n{X_embedded[:, 0, :]}\\n')\n",
    "\n",
    "# Knowing above we can concatenate all embeddings for each character in each sequence\n",
    "print(f'torch.concat + tensor list=\\n{torch.concat([X_embedded[:, i, :] for i in range(X_embedded.shape[1])], dim=1)}\\n')\n",
    "\n",
    "# We can also improve it by using torch.unbind which will return list of tensors and yield same result as for loop\n",
    "print(f'torch.concat + torch.unbind=\\n{torch.concat(torch.unbind(X_embedded, dim=1), dim=1)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape=torch.Size([16]), a=tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\n",
      "a.view(2, 8)=\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15]])\n",
      "\n",
      "a.view(4, 4)=\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "\n",
      "a.view(2, 2, 4)=\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# But it turns out there is another great function in pytorch called torch.view\n",
    "# Lets have some fun with it\n",
    "\n",
    "a = torch.arange(16) # 1D tensor with 16 elements\n",
    "print(f'a.shape={a.shape}, a={a}\\n')\n",
    "print(f'a.view(2, 8)=\\n{a.view(2, 8)}\\n')\n",
    "print(f'a.view(4, 4)=\\n{a.view(4, 4)}\\n')\n",
    "print(f'a.view(2, 2, 4)=\\n{a.view(2, 2, 4)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embedded_flat.shape=torch.Size([5, 6])\n",
      "\n",
      "X_embedded_flat=\n",
      "tensor([[ 0.1912,  0.2553,  0.1912,  0.2553,  0.1912,  0.2553],\n",
      "        [ 0.1912,  0.2553,  0.1912,  0.2553,  2.1247,  0.4705],\n",
      "        [ 0.1912,  0.2553,  2.1247,  0.4705, -0.3938, -0.0033],\n",
      "        [ 2.1247,  0.4705, -0.3938, -0.0033, -0.3938, -0.0033],\n",
      "        [-0.3938, -0.0033, -0.3938, -0.0033,  0.6780, -2.8017]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Knowing above we can use torch.view to transform our 5x3x2 tensor into 5x6\n",
    "# Basically we just keep first dimension and merge all other dimensions into one\n",
    "# Also it looks like 6 is the only possible value for second dimension because 3*2=6\n",
    "# Knowing that we can use -1 for second dimension and pytorch will figure out the rest\n",
    "X_embedded_flat = X_embedded.view(-1, 6)\n",
    "print(f'X_embedded_flat.shape={X_embedded_flat.shape}\\n')\n",
    "print(f'X_embedded_flat=\\n{X_embedded_flat}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First layer multiplication to get outputs from hidden layer\n",
    "Our current desing of network is: 6 inputs which represents 2 dimensional embeddings for 3 character long sequence that go into 100 neurons in hidden layer and what we get is 100 outputs for each input sequence.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h.shape=torch.Size([5, 100])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h = torch.tanh(X_embedded_flat @ W1 + b1) \n",
    "# + b1 is broadcasted to match shape of X_embedded_flat @ W1\n",
    "# 5, 100\n",
    "# 1, 100 - broadcasting will align on the right, create fake dimension on the left\n",
    "# and will be copy b1 values 5 times to match shape of (X_embedded_flat @ W1) and perform elementwise addition\n",
    "# and will result of adding 5x100 matrix to 5x100 matrix with copied values from b1\n",
    "print(f'h.shape={h.shape}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "Takes 100 outputs from hidden layer and produces output that represent one_hot_encoding for our characters set for each input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((hidden_layer_size, chars_count))\n",
    "b2 = torch.randn(chars_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forwad pass for output layer\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# calculate output probabilities (the old way, manualy calculating softmax)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "# but there is faster way to calculate probabilities - use torch.nn.functional.softmax\n",
    "# and this is equivalent to above calculation\n",
    "probs = F.softmax(logits, dim=1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1298e-01, 2.9922e-10, 5.5552e-09, 2.3597e-04, 1.8006e-14, 5.0978e-11,\n",
       "        2.5228e-04, 2.1771e-05, 5.6496e-09, 4.5508e-03, 1.0967e-09, 5.4565e-02,\n",
       "        1.2870e-02, 1.2076e-05, 1.7189e-01, 2.7744e-07, 1.3170e-03, 2.3272e-01,\n",
       "        9.8346e-09, 2.2946e-04, 2.1999e-07, 2.4608e-07, 2.6029e-07, 2.6963e-02,\n",
       "        8.7580e-07, 2.0536e-06, 6.2815e-09, 8.1386e-02])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And what we see here is output character probabilities for second input sequence\n",
    "probs[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training = 23.061771392822266\n"
     ]
    }
   ],
   "source": [
    "# iterator over input sequences\n",
    "it = torch.arange(X_embedded.shape[0])\n",
    "\n",
    "# extract probabilities for target characters\n",
    "target_probs = probs[it, Y]\n",
    "\n",
    "# calculate loss as average negative log likelihood\n",
    "loss = -torch.log(target_probs).mean()\n",
    "\n",
    "print(f'Loss before training = {loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep it short and clean without outputs\n",
    "Above code show detail example of how MLP language model works with one input word.  Since I already went step by step with relevant outputs and example is clear next step is to make this code compact and efficient so it can be used for training and evaluation of language model for whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters = 3584\n"
     ]
    }
   ],
   "source": [
    "# Build dataset for training\n",
    "context_length = 3\n",
    "X, Y = build_dataset(words, context_length)\n",
    "\n",
    "# Create generator for reproducibility\n",
    "g = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Create embedding lookup table with size chars_count\n",
    "embedding_size = 2\n",
    "C = torch.randn((chars_count, embedding_size), requires_grad=True, generator=g)\n",
    "\n",
    "# Embed input sequences\n",
    "X_embedded = C[X]\n",
    "\n",
    "# Flatten embedded input sequences, to keep embeddings in one dimension\n",
    "input_layer_size = X_embedded.shape[1] * X_embedded.shape[2] # context_length * embedding_size\n",
    "X_embedded_flat = X_embedded.view(-1, input_layer_size)\n",
    "\n",
    "# Create first neuron layer\n",
    "hidden_layer_size = 100\n",
    "W1 = torch.randn((input_layer_size, hidden_layer_size), requires_grad=True, generator=g)\n",
    "b1 = torch.randn(hidden_layer_size, requires_grad=True, generator=g)\n",
    "\n",
    "# Create second neuron layer\n",
    "W2 = torch.randn((hidden_layer_size, chars_count), requires_grad=True, generator=g)\n",
    "b2 = torch.randn(chars_count, requires_grad=True, generator=g)\n",
    "\n",
    "# Parameters of the network\n",
    "params = [C, W1, b1, W2, b2]\n",
    "print(f'Number of parameters = {sum([p.numel() for p in params])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss = 15.284095764160156\n",
      "Loss after training = 3.632993221282959\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    h1 = torch.tanh(X_embedded_flat @ W1 + b1)\n",
    "    logits = h1 @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    # Backward pass\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Update parameters\n",
    "    for p in params:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    # Print loss every 10th epoch\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, loss = {loss}')\n",
    "        \n",
    "print(f'Loss after training = {loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy vs manual calculation\n",
    "\n",
    "Calculate mean negative log likelihood based on softmax made of logits\n",
    "using torch.nn.functional.cross_entropy() which is give same result as:\n",
    "```python\n",
    "counts = logits.exp() # this may cause trobles if logits are too big we will get nan, since exp for big numbers is inf\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "loss = -torch.log(probs[range(len(Y)), Y]).mean()\n",
    "```\n",
    "- but much meory efficient because we do not create intermediate tensors\n",
    "- and it is safe in case of possible big logits because internally it ofsets them by max logit value\n",
    "- and much faster and... also makes backpropagation easier :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shiyaa\n",
      "ciiyt\n",
      "kmiieaazknw\n",
      "iimneslilemar\n",
      "cesliauh\n",
      "aueaat\n",
      "creaac\n",
      "mai\n",
      "alan\n",
      "limaa\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    out = []\n",
    "    \n",
    "    context = [stoi[start_token]] * 3\n",
    "    while True:\n",
    "        embedding = C[torch.tensor([context])]\n",
    "        embedding_flat = embedding.view(-1, 6)\n",
    "        h1 = torch.tanh(embedding_flat @ W1 + b1)\n",
    "        logits = h1 @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # sample next character\n",
    "        ix = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        context = context[1:] + [ix]\n",
    "        if ix == stoi[end_token]:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
