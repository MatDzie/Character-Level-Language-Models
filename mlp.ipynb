{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Character Level Multi Layer Perceptron Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '<',\n",
       " 27: '>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create lookup table for converting characters to indices\n",
    "chars = sorted(list(set(''.join(words)))) # all unique characters in the dataset\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # string to index\n",
    "\n",
    "# manually enumerate start and end token since they are not visible in the dataset\n",
    "start_token = '<'\n",
    "end_token = '>'\n",
    "stoi[start_token] = 0\n",
    "stoi[end_token] = len(stoi)\n",
    "\n",
    "# total number of unique characters plus start and end token\n",
    "chars_count = len(stoi)\n",
    "\n",
    "# index to string\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "itos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset\n",
    "Based on given list of words creates input tensor with sequence of characters  with length equal to 'context_length' and target tensor with next character in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, context_length=3):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [stoi[start_token]] * context_length # context window padded with start token\n",
    "        for ch in w + end_token:\n",
    "            ix = stoi[ch]\n",
    "            X.append(context) # for given context ...\n",
    "            Y.append(ix) # ... the next character is the target\n",
    "            context = context[1:] + [ix] # crop and append - sliding window\n",
    "    \n",
    "    return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X (input) and Y (output/target/label) tensors\n",
    "X, Y = build_dataset(words[:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding using index C[15]=tensor([-1.4355,  0.3942])\n",
      "Embedding using one-hot encoding and matrix multiplication=tensor([[-1.4355,  0.3942]])\n",
      "Embedding for sequence of three characters C[torch.tensor([15, 20, 5])]=\n",
      "tensor([[-1.4355,  0.3942],\n",
      "        [-1.2593,  0.7810],\n",
      "        [ 0.7148,  0.1095]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix of embeddings\n",
    "embedding_dim = 2\n",
    "C = torch.randn((chars_count, embedding_dim))\n",
    "\n",
    "# Example of embedding single character with index 15\n",
    "# since our matrix of embeddings is the same size as the number of characters\n",
    "# we can simply use the index of the character to get its embedding\n",
    "embedding = C[15]\n",
    "print(f'Embedding using index C[15]={embedding}')\n",
    "\n",
    "# Alternative approach would be to one-hot encode the character and then multiply it by the embedding matrix\n",
    "# this will give same result because encoded vector will have only one non-zero value equal to 1\n",
    "# and this will simply act as a mask for the embedding matrix\n",
    "embedding = F.one_hot(torch.tensor([15]), num_classes=chars_count).float() @ C\n",
    "print(f'Embedding using one-hot encoding and matrix multiplication={embedding}')\n",
    "\n",
    "\n",
    "# For rest of of notebook I will use index based approach because it is more efficient\n",
    "# also thanks to python semantics we can easly retrieve embedding for whole sequence of characters\n",
    "embedding = C[torch.tensor([15, 20, 5])]\n",
    "print(f'Embedding for sequence of three characters C[torch.tensor([15, 20, 5])]=\\n{embedding}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
